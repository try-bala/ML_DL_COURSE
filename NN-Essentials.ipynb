{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions \n",
    "\n",
    "the idealization of how a varying quantity depends on another quantity.\n",
    "\n",
    "A function is a process or a relation that associates each element x  to a  element y . \n",
    "\n",
    "If the function is called f, this relation is denoted y = f (x)  the element x is the argument or input of the function, and y is the value of the function, the output.\n",
    "\n",
    "A function is uniquely represented by its graph which is the set of all pairs (x, f (x)).\n",
    "\n",
    "<img src='myimages/function.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model\n",
    "\n",
    "A model is the acquired experience.\n",
    "The slang adopted by its user community, or the definitions used in published academic papers, which can vary widely from journal to journal.\n",
    "\n",
    "The term ML model refers to the model artifact that is created by the training process.\n",
    "\n",
    "A model is usually specified by mathematical equations/expression that relate one or more random variables/parameters and possibly other non-random variables/parameters for each prediction, classification and reinforcement categories respectively.This equations/expression is embedded in the single term as a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives \n",
    "\n",
    "Derivatives are a fundamental tool of calculus.\n",
    "\n",
    "The derivative of a function of a real variable measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value). \n",
    "For example, the derivative of the position of a moving object with respect to time is the object's velocity: this measures how quickly the position of the object changes when time advances.\n",
    "\n",
    "The action of computing a derivative is called Differentiation. \n",
    "The derivative of a function y = f(x) of a variable x is a measure of the rate at which the value y of the function changes with respect to the change of the variable x. It is called the derivative of f with respect to x.\n",
    "\n",
    "when y is a linear function of x, meaning that the graph of y is a line. In this case, y = f(x) = mx + b, for real numbers m and b, and the slope m is given by\n",
    "\n",
    "<img src='myimages/derivative_eqn.png'>\n",
    "\n",
    "The derivative of a function of a single variable at a chosen input value, when it exists, is the slope of the tangent line to the graph of the function at that point.\n",
    "\n",
    "<img src='myimages/derivative-slope.png'>\n",
    "\n",
    "The tangent line is the best linear approximation of the function near that input value. For this reason, the derivative is often described as the \"instantaneous rate of change\", the ratio of the instantaneous change in the dependent variable to that of the independent variable.\n",
    "\n",
    "<img src='myimages/slope.png'>\n",
    "\n",
    "Derivatives may be generalized to functions of several real variables. In this generalization, the derivative is reinterpreted as a linear transformation whose graph is (after an appropriate translation) the best linear approximation to the graph of the original function.\n",
    "\n",
    "The Jacobian matrix is the matrix that represents this linear transformation with respect to the basis given by the choice of independent and dependent variables. \n",
    "\n",
    " It can be calculated in terms of the partial derivatives with respect to the independent variables. For a real-valued function of several variables, the Jacobian matrix reduces to the gradient vector.\n",
    "    \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation : \n",
    "\n",
    "Backpropagation is the key algorithm that makes training deep models computationally tractable.\n",
    "\n",
    "Beyond its use in deep learning, backpropagation is a powerful computational tool in many other areas, ranging from weather forecasting to analyzing numerical stability – it just goes by different names.\n",
    "\n",
    "The general, application independent, name is “reverse-mode differentiation.”\n",
    "\n",
    "Fundamentally, it’s a technique for calculating derivatives quickly. And it’s an essential trick to have in your bag, not only in deep learning, but in a wide variety of numerical computing situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graphs\n",
    "\n",
    "Computational graphs are a nice way to think about mathematical expressions. For example, consider the expression e=(a+b)∗(b+1). There are three operations: two additions and one multiplication. To help us talk about this, let’s introduce two intermediary variables, c and d so that every function’s output has a variable. We now have:\n",
    "\n",
    "c=a+b\n",
    "\n",
    "d=b+1\n",
    "\n",
    "e=c∗d\n",
    "\n",
    "To create a computational graph, we make each of these operations, along with the input variables, into nodes. When one node’s value is the input to another node, an arrow goes from one to another.\n",
    "\n",
    "<img src=\"myimages/computational-graph.png\">\n",
    "\n",
    "We can evaluate the expression by setting the input variables to certain values and computing nodes up through the graph. For example, let’s set a=2 and b=1:\n",
    "\n",
    "<img src=\"myimages/comp-graph1.png\">\n",
    "\n",
    "The expression evaluates to 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives and Partial Derivatives on Computational Graph\n",
    "\n",
    "If a directly affects c, then we want to know how it affects c. If a changes a little bit, how does c change? We call this the partial derivative of c with respect to a.\n",
    "\n",
    "To evaluate the partial derivatives in this graph, we need the sum rule and the product rule:\n",
    "\n",
    "<img src=\"myimages/partial-derivative.png\">\n",
    "\n",
    "Below, the graph has the derivative on each edge labeled.\n",
    "\n",
    "<img src=\"myimages/derivative-graph.png\">\n",
    "\n",
    "What if we want to understand how nodes that aren’t directly connected affect each other? Let’s consider how e is affected by a. If we change a at a speed of 1, c also changes at a speed of 1. In turn, c changing at a speed of 1 causes e to change at a speed of 2. So e changes at a rate of 1∗2 with respect to a.\n",
    "\n",
    "The general rule is to sum over all possible paths from one node to the other, multiplying the derivatives on each edge of the path together. For example, to get the derivative of e with respect to b we get:\n",
    "\n",
    "<img src=\"myimages/chain-rule.png\">\n",
    "\n",
    "This accounts for how b affects e through c and also how it affects it through d.\n",
    "\n",
    "This general “sum over paths” rule is just a different way of thinking about the multivariate chain rule.\n",
    "\n",
    "\n",
    "<img src=\"myimages/computation-graph-hinge.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivatives\n",
    "\n",
    "The derivative of a function of a real variable measures the sensitivity to change of the function value (output value) with respect to a change in its argument (input value).\n",
    "\n",
    "<img src=\"myimages/unit-derivative.png\">\n",
    "\n",
    "<img src=\"myimages/diff.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent \n",
    "\n",
    "Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient.\n",
    "\n",
    "<img src=\"myimages/gradient-descent.png\">\n",
    "\n",
    "In mulitple  dimension gradient is a vector of partial derivative of all the dimension wrt the function.\n",
    "\n",
    "One dimension gradient descent plot :\n",
    "\n",
    "<img src=\"myimages/grad-descent-plot.png\">\n",
    "\n",
    "Learning rate : The size of these steps is called the learning rate.\n",
    "\n",
    "Cost Function : A Loss Functions tells us “how good” our model is at making predictions for a given set of parameters. The slope of this curve tells us how to update our parameters to make the model more accurate.\n",
    "\n",
    "Given the cost function:\n",
    "\n",
    "There are two parameters in our cost function we can control: m (weight) and b (bias). \n",
    "\n",
    "Since we need to consider the impact each one has on the final prediction, we need to use partial derivatives. \n",
    "\n",
    "We calculate the partial derivatives of the cost function with respect to each parameter and store the results in a gradient.\n",
    "\n",
    "<img src=\"myimages/grad-descent-cost.png\">\n",
    "\n",
    "The gradient can be calculated as:\n",
    "\n",
    "<img src=\"myimages/gradient-calculation.png\">\n",
    "\n",
    "Updating the parameter :\n",
    "\n",
    "To solve for the gradient, we iterate through our data points using our new m and n values and compute the partial derivatives. This new gradient tells us the slope of our cost function at our current position (current parameter values) and the direction we should move to update our parameters. The size of our update is controlled by the learning rate.\n",
    "\n",
    "m = m - sum(1/N(lr*m_derivative ))\n",
    "\n",
    "b = b - sum(1/N(lr*b_derivative ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain Rule\n",
    "\n",
    "The chain rule states that the derivative of f(g(x)) is f'(g(x))⋅g'(x). \n",
    "\n",
    "In other words, it helps us differentiate *composite functions*. For example, sin(x²) is a composite function because it can be constructed as f(g(x)) for f(x)=sin(x) and g(x)=x². Using the chain rule and the derivatives of sin(x) and x², we can then find the derivative of sin(x²)\n",
    "\n",
    "If a variable z depends on the variable y, which itself depends on the variable x, so that y and z are therefore dependent variables, then z, via the intermediate variable of y, depends on x as well. \n",
    "\n",
    "The chain rule then states,\n",
    "\n",
    "<img src=\"myimages/chain-rule-notation.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in practice\n",
    "\n",
    "As discussed above, the exact form of the updates depends on both the chosen cost function and each layer's chosen nonlinearity.  The following two table lists the some common choices for nonlinearities and the required partial derivative for deriving the gradient for each layer:\n",
    "\n",
    "| Nonlinearity | $a^m = \\sigma^m(z^m)$ | $\\frac{\\partial a^m}{\\partial z^m}$ | Notes |\n",
    "|--------------|---|---|---|\n",
    "| Sigmoid      | $\\frac{1}{1 + e^{z^m}}$ | $\\sigma^m(z^m)(1 - \\sigma^m(z^m)) = a^m(1 - a^m)$ | \"Squashes\" any input to the range $[0, 1]$ |\n",
    "| Tanh         | $\\frac{e^{z^m} - e^{-z^m}}{e^{z^m} + e^{-z^m}}$ | $1 - (\\sigma^m(z^m))^2 = 1 - (a^m)^2$ | Equivalent, up to scaling, to the sigmoid function |\n",
    "| ReLU         | $\\max(0, z^m)$ | $0, z^m < 0;\\; 1, z^m \\ge 0$ | Commonly used in neural networks with many layers|\n",
    "\n",
    "<img src=\"myimages/sigmoid-fun-derivative-plot.png\">\n",
    "<img src=\"myimages/relu-func-derivative-plot.png\">\n",
    "<img src=\"myimages/all-sct-fun-derivative-func-plot.png\">\n",
    "\n",
    "\n",
    "Similarly, the following table collects some common cost functions and the partial derivative needed to compute the gradient for the final layer:\n",
    "\n",
    "| Cost Function | $C$                                  | $\\frac{\\partial C}{\\partial a^L}$ | Notes |\n",
    "|---------------|--------------------------------------|-----------------------------------|---|\n",
    "| Squared Error | $\\frac{1}{2}(y - a^L)^\\top(y - a^L)$ | $y - a^L$                         | Commonly used when the output is not constrained to a specific range |\n",
    "| Cross-Entropy | $(y - 1)\\log(1 - a^L) - y\\log(a^L)$  | $\\frac{a^L - y}{a^L(1 - a^L)}$    | Commonly used for binary classification tasks; can yield faster convergence |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function and derivative plots\n",
    "<img src=\"myimages/backprop-example1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example2 : \n",
    "\n",
    "#### f(x,y,z,w) = 2*((x*y) + max(z,w))\n",
    "\n",
    "<img src=\"myimages/backprop-explanation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exercise :  f(w,x) = $\\frac{1}{1+e^−(w0x0+w1x1+w2)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NN\n",
    "# Functions : \n",
    "# Derivatives \n",
    "# Few functions : sigmoid , relu \n",
    "# Few unit fuctions derivative : sum , mul , exponential , polynomial\n",
    "# Few complex functions derivatives : sigmoid \n",
    "# functions plot and derivatives plot\n",
    "# Parameter Update\n",
    "# Gradient Descent\n",
    "# Learning rate\n",
    "# computational Graphs\n",
    "# local gradient\n",
    "# chain rule\n",
    "# backprop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
